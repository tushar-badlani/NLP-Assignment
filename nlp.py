# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NQJLaMBkgINRMSaUgakoI3_o24nNc21t
"""




import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import nltk
from nltk.tokenize import word_tokenize
import re
#nltk.download('punkt')

df = pd.read_excel('Input.xlsx')
df.head()

for row in df.iterrows():
  id =row[1]['URL_ID']
  url = row[1]['URL']
  print(url)
  try:
    response =requests.get(url)
  except Exception as e:
    print(e)
    print(f"Cant get response of id {id}")
    continue

  soup = BeautifulSoup(response.content, 'html.parser')
  try:
    title = soup.find('h1').get_text()
  except:
    print(f'Cant get title of id {id}')
    title = "none"

  text = ""

  for p in soup.find_all('p'):
    text += p.get_text()

  file_name = f"/articles/{id}"
  with open(file_name, 'w') as file:
    file.write(title + '\n' + text)

text_dir = "/articles"
stop_dir = "/StopWords"
dict_dir = "/MasterDictionary"

stopwords_list = []
for stopwords_file in os.listdir(stop_dir):
    with open(os.path.join(stop_dir, stopwords_file), 'r', encoding='ISO-8859-1') as f:
        stopwords_list.extend(f.read().lower().splitlines())

print(stopwords_list)

articles = []


for text_file in os.listdir(text_dir):
  with open(os.path.join(text_dir,text_file),'r') as f:
    text = f.read()

    words = word_tokenize(text)

    filtered_text = [word for word in words if word.lower() not in stopwords_list]

    articles.append(filtered_text)

print(len(articles))

pos = []
neg = []

for files in os.listdir(dict_dir):
  if files =='positive-words.txt':
    with open(os.path.join(dict_dir,files),'r',encoding='ISO-8859-1') as f:
      pos.extend(f.read().lower().splitlines())
  else:
    with open(os.path.join(dict_dir,files),'r',encoding='ISO-8859-1') as f:
      neg.extend(f.read().lower().splitlines())

positive_words = []
positive_score = []
negative_words = []
negative_score = []
polarity_score = []
subjectivity_score = []

for i in range(len(articles)):
  positive_words.append([word for word in articles[i] if word.lower() in pos])
  negative_words.append([word for word in articles[i] if word.lower() in neg])
  positive_score.append(len(positive_words[i]))
  negative_score.append(len(negative_words[i]))
  polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))
  subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(articles[i])) + 0.000001))

avg_sentence_length = []
Percentage_of_Complex_words  =  []
Fog_Index = []
complex_word_count =  []
avg_syllable_word_count =[]


def calculate(file):
  with open(os.path.join(text_dir, file),'r') as f:
    text = f.read()

    text = re.sub(r'[^\w\s.]','',text)

    sentences = text.split('.')

    num_sentences = len(sentences)

    words = [word  for word in text.split() if word.lower() not in stopwords_list ]
    num_words = len(words)

    complex_words = []
    for word in words:
      vowels = 'aeiou'
      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
      if syllable_count_word > 2:
        complex_words.append(word)

    syllable_count = 0
    syllable_words =[]
    for word in words:
      if word.endswith('es'):
        word = word[:-2]
      elif word.endswith('ed'):
        word = word[:-2]
      vowels = 'aeiou'
      syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)
      if syllable_count_word >= 1:
        syllable_words.append(word)
        syllable_count += syllable_count_word

    avg_sentence_len = num_words / num_sentences
    avg_syllable_word_count = syllable_count / len(syllable_words)
    Percent_Complex_words  =  len(complex_words) / num_words
    Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)

    return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words),avg_syllable_word_count


for file in os.listdir(text_dir):
  x,y,z,a,b = calculate(file)
  avg_sentence_length.append(x)
  Percentage_of_Complex_words.append(y)
  Fog_Index.append(z)
  complex_word_count.append(a)
  avg_syllable_word_count.append(b)

def cleaned_words(file):
  with open(os.path.join(text_dir,file), 'r') as f:
    text = f.read()
    text = re.sub(r'[^\w\s]', '' , text)
    words = [word  for word in text.split() if word.lower() not in stopwords_list]
    length = sum(len(word) for word in words)
    average_word_length = length / len(words)
  return len(words),average_word_length

word_count = []
average_word_length = []
for file in os.listdir(text_dir):
  x, y = cleaned_words(file)
  word_count.append(x)
  average_word_length.append(y)


def count_personal_pronouns(file):
  with open(os.path.join(text_dir,file), 'r') as f:
    text = f.read()
    personal_pronouns = ["I", "we", "my", "ours", "us", "We", "My", "Ours", "Us"]
    count = 0
    for pronoun in personal_pronouns:
      count += len(re.findall(r"\b" + pronoun + r"\b", text))
  return count


pp_count = []
for file in os.listdir(text_dir):
  x = count_personal_pronouns(file)
  pp_count.append(x)

variables = [positive_score,
            negative_score,
            polarity_score,
            subjectivity_score,
            avg_sentence_length,
            Percentage_of_Complex_words,
            Fog_Index,
            avg_sentence_length,
            complex_word_count,
            word_count,
            avg_syllable_word_count,
            pp_count,
            average_word_length]



output = pd.read_excel('Output Data Structure.xlsx')

for i, var in enumerate(variables):
  output.iloc[:,i+2] = var


output.head()

output.to_csv("Output.csv")